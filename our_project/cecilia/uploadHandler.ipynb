{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load \n",
    "from sqlite3 import connect\n",
    "from pandas import DataFrame\n",
    "from pandas import read_csv\n",
    "from rdflib import Graph, URIRef, Literal, RDF \n",
    "from rdflib.plugins.stores.sparqlstore import SPARQLUpdateStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Handler(object): #this is the first class, all the others derive from this one \n",
    "\n",
    "    #creating the class \n",
    "    def __init__(self, dbPathOrUrl : str):\n",
    "        self.dbPathOrUrl = dbPathOrUrl\n",
    "\n",
    "    #creating the methods \n",
    "    def getDbPathOrUrl(self): \n",
    "        return self.dbPathOrUrl \n",
    "\n",
    "    def setDbPathOrUrl(self, pathOrUrl : str): #: boolean \n",
    "        self.dbPathOrUrl = pathOrUrl\n",
    "        return True\n",
    "\n",
    "class UploadHandler(Handler):\n",
    "\n",
    "    def pushDataToDb(self, path: str):  #self implied \n",
    "        if path.lower().endswith(\".csv\"): \n",
    "            handler = JournalUploadHandler(self.dbPathOrUrl)\n",
    "            return handler.journalUpload(path) #calling the method after I called the subclass\n",
    "        elif path.lower().endswith(\".json\"): \n",
    "            handler = CategoryUploadHandler(self.dbPathOrUrl)\n",
    "            return handler.categoryUpload(path)\n",
    "        else: \n",
    "            return False \n",
    "\n",
    "\n",
    "#first case: the path is of the relational database the json file\n",
    "\n",
    "class CategoryUploadHandler(UploadHandler): \n",
    "    \n",
    "    def categoryUpload(self, path: str): \n",
    "\n",
    "        #creating the database \n",
    "        with connect(self.dbPathOrUrl) as con: \n",
    "            con.commit() #commit the current transactions to the database  \n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as c: \n",
    "            json_data = load(c) #reading the file \n",
    "\n",
    "            identifier_list = []\n",
    "\n",
    "            category_mapping_dict = {} #using it to keep track of what we have\n",
    "            categories_list = []\n",
    "\n",
    "            area_mapping_dict = {}\n",
    "            area_list = []\n",
    "\n",
    "            #internal identifier of all the items \n",
    "            for idx, item in enumerate(json_data): \n",
    "                item_internal_id = (\"item-\" + str(idx))\n",
    "            \n",
    "            #1. creating internal ids for each element: identifiers \n",
    "                identifiers = item[\"identifiers\"] #selecting the identifiers  \n",
    "\n",
    "                #iterating through the identifiers indise the bigger loop of items\n",
    "                for idx, row in enumerate(identifiers): #i use the iteration because there are more than one in some cases \n",
    "                    identifiers_internal_id = (\"internal_id-\") +  str(idx)\n",
    "                    identifier_list.append({\n",
    "                            \"item_internal_id\": item_internal_id,\n",
    "                            \"identifier\": identifiers_internal_id,\n",
    "                            \"identifiers\": identifiers\n",
    "                            })  #associating the data, with the internal id of the single category but also to the identifies of the whole item so that it's easier to query \n",
    "\n",
    "                #2. creating internal ids for the categories, this is trickier because they have more than one value and they can have same id\n",
    "                #i have to iterate thourg everything but check if the \"id\" is the same, so it's useful to use a dictionary \n",
    "                categories = item[\"categories\"]\n",
    "\n",
    "                for idx, row in enumerate(categories): #appunto per me, scrivere cat_id = category[\"id\"] non ha senso perchè category è una lista di un dizionario, io devo internere come dizionario il singolo item \n",
    "                    cat_id = row[\"id\"]\n",
    "\n",
    "                    if cat_id not in category_mapping_dict: #checking if the category is not already in the dictionary \n",
    "                        category_id_internal_id = (\"category_id-\") + str(idx)\n",
    "                        category_mapping_dict[cat_id] = (category_id_internal_id)\n",
    "                    else: \n",
    "                        category_id_internal_id = category_mapping_dict[cat_id] #if it's already inside the dict consider the original id \n",
    "\n",
    "                    categories_list.append({\n",
    "                        \"item_internal_id\": item_internal_id,\n",
    "                        \"category_internal_id\" : category_id_internal_id,\n",
    "                        \"id\": cat_id,\n",
    "                        \"quartile\": row[\"quartile\"]\n",
    "                    })\n",
    "            \n",
    "                #3. creating internal ids for areas, this is the same but without any more value \n",
    "                areas = item[\"areas\"]\n",
    "\n",
    "                for idx, row in enumerate(areas): \n",
    "                    area_section = areas[0]\n",
    "                    if area_section not in area_mapping_dict: \n",
    "                       area_id = ((\"areas-\") + str(idx))\n",
    "                       area_mapping_dict[area_section] = area_id\n",
    "                    else: \n",
    "                        area_id = area_mapping_dict[area_section]\n",
    "                \n",
    "                    area_list.append({\n",
    "                        \"item_internal_id\": item_internal_id, \n",
    "                        \"area_internal_id\": area_id,\n",
    "                        \"area\": area_section\n",
    "                    })\n",
    "\n",
    "            #converting the data in dataframes \n",
    "            identifiers_df = DataFrame(identifier_list)\n",
    "            categories_df = DataFrame(categories_list)\n",
    "            areas_df = DataFrame(area_list)\n",
    "\n",
    "        #adding them to the database \n",
    "        with connect(self.dbPathOrUrl) as con:\n",
    "            identifiers_df.to_sql(\"identifiers\", con, if_exists=\"replace\", index=False)\n",
    "            categories_df.to_sql(\"categories\", con, if_exists=\"replace\", index=False)\n",
    "            areas_df.to_sql(\"areas\", con, if_exists=\"replace\", index=False)\n",
    "\n",
    "            \n",
    "#second case: the path is the one of a graph database, the csv file\n",
    "\n",
    "\n",
    "class JournalUploadHandler(UploadHandler): \n",
    "  \n",
    "    def journalUpload(self, path: str):  \n",
    "        my_graph = Graph() #creating the database\n",
    "\n",
    "        #classes\n",
    "        IdentifiableEntity = URIRef(\"https://schema.org/Thing\") #I made this super generic because id is already an attribute\n",
    "        Journal = URIRef(\"https://schema.org/Periodical\") \n",
    "        Category = URIRef(\"https://schema.org/category\")\n",
    "        Area = URIRef(\"https://www.wikidata.org/wiki/Q26256810\") #I found the one of the topic because area has a different interpretation as more of a physical meaning \n",
    "\n",
    "        #predicate \n",
    "        hasCategory = URIRef(\"http://purl.org/dc/terms/subject\")\n",
    "        hasArea = URIRef(\"https://schema.org/about\")\n",
    "\n",
    "        #attributes related to classes \n",
    "        id = URIRef(\"https://schema.org/identifier\")\n",
    "        title = URIRef(\"https://schema.org/title\")\n",
    "        languages = URIRef(\"https://schema.org/inLanguage\") \n",
    "        publisher = URIRef(\"https://schema.org/publisher\")\n",
    "        doajSeal = URIRef(\"https://schema.org/Certification\") \n",
    "        licence = URIRef(\"https://schema.org/license\")\n",
    "        apc = URIRef(\"https://schema.org/isAccessibleForFree\")\n",
    "        quartile = URIRef(\"https://schema.org/ratingValue\") #to revise is it useful? \n",
    "        #the impact of the journal in the respecitive field so i use the ranking attribute\n",
    "\n",
    "\n",
    "        #reading the csv  Journal title,Journal ISSN (print version),Journal EISSN (online version),Languages in which the journal accepts manuscripts,Publisher,DOAJ Seal,Journal license,APC\n",
    "        journals = read_csv(path, \n",
    "                            keep_default_na=False, \n",
    "                            names=[\"Journal title\", \"Journal ISSN\", \"Journal EISSN\", \"Languages\", \"Publisher\", \"DOAJ Seal\", \"Journal License\", \"APC\"],\n",
    "                            dtype={\n",
    "                                \"Journal title\": \"string\",\n",
    "                                \"Journal ISSN\": \"string\",\n",
    "                                \"Journal EISSN\": \"string\",\n",
    "                                \"Languages\": \"string\",\n",
    "                                \"Publisher\": \"string\",\n",
    "                                \"DOAJ Seal\": \"string\",\n",
    "                                \"Journal License\" : \"string\",\n",
    "                                \"APC\": \"string\"\n",
    "                            })\n",
    "        print(journals.columns)\n",
    "        #giving unique identifiers \n",
    "        base_url = \"https://comp-data.github.io/res/\"\n",
    "        \n",
    "        for idx, row in journals.iterrows(): \n",
    "            local_id = \"journal-\" + str(idx)\n",
    "            subj = URIRef(base_url + local_id)#new local identifiers for each item in the graph database \n",
    "\n",
    "            my_graph.add(((subj, RDF.type, Journal))) #the subject of the row is a journal \n",
    "            #checking every category in the row (which is none other than a panda Series, so a list of vocabularies)\n",
    "            if row[\"Journal title\"]: \n",
    "                my_graph.add((subj, title, Literal(row[\"Journal title\"])))\n",
    "            if row[\"Journal ISSN\"]: \n",
    "                my_graph.add((subj, id, Literal(row[\"Journal ISSN\"])))\n",
    "                #NEED TO DECIDE IF WE WANT TO CONSIDER BOTH AS ID, OR TO SEPATATE THEM (https://schema.org/issn) \n",
    "            if row[\"Journal EISSN\"]: \n",
    "                my_graph.add((subj, id, Literal(row[\"Journal EISSN\"])))\n",
    "            if row[\"Languages\"]: \n",
    "                language_string = row[\"Languages\"] #1. taking in consideration the whole row\n",
    "                language_list = language_string.split(\",\") #as indicated in the F.A.Q on the github they are separated with a comma but inside quotes of course \",\", so I separate each item \n",
    "                for language in language_list: \n",
    "                    language = language.strip() #to delete whitespaces and facilitate the query \n",
    "                    my_graph.add((subj, languages, Literal(language)))\n",
    "                \n",
    "            if row[\"Publisher\"]: \n",
    "                my_graph.add((subj, publisher, Literal(row[\"Publisher\"])))\n",
    "            if row[\"DOAJ Seal\"]: \n",
    "                my_graph.add((subj, doajSeal, Literal(row[\"DOAJ Seal\"])))\n",
    "            if row[\"Journal License\"]: \n",
    "                my_graph.add((subj, licence, Literal(row[\"Journal License\"])))\n",
    "            if row[\"APC\"]: \n",
    "                my_graph.add((subj, apc, Literal(row[\"APC\"])))\n",
    "\n",
    "        #opening the connection to upload the graph \n",
    "        store = SPARQLUpdateStore() #initializing it as an object ù\n",
    "        print(store)\n",
    "        try: \n",
    "            store.open((self.dbPathOrUrl, self.dbPathOrUrl))\n",
    "\n",
    "            for triple in my_graph.triples((None, None, None)): \n",
    "                store.add(triple)\n",
    "            \n",
    "            store.serialize(destination=path, format=\"turtle\")\n",
    "\n",
    "            #closing the connection when we finish \n",
    "            \n",
    "            store.close()\n",
    "        # endpoint =  self.dbPathOrUrl the endopoint is he url or path of the database \n",
    "\n",
    "        except Exception as e: \n",
    "            print (\"Problems with the Blazegraph connection: \", e) #\n",
    "\n",
    "\n",
    "\n",
    "        #closing the connection when we finish \n",
    "        store.close()\n",
    "        #SHOULD WE WRITE SOMETHING IF IT DOESN'T WORK? print(\"something went wrong\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymantic import sparql\n",
    "\n",
    "server = sparql.SPARQLServer('http://127.0.0.1:9999/bigdata/sparql')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Handler(dbPathOrUrl=\"http://127.0.0.1:9999/\")\n",
    "uphandler = UploadHandler(\"http://127.0.0.1:9999/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Journal title', 'Journal ISSN', 'Journal EISSN', 'Languages',\n",
      "       'Publisher', 'DOAJ Seal', 'Journal License', 'APC'],\n",
      "      dtype='object')\n",
      "<rdflib.plugins.stores.sparqlstore.SPARQLUpdateStore object at 0x12e6fd710>\n",
      "Problems with the Blazegraph connection:  unsupported operand type(s) for +: 'UploadHandler' and 'str'\n"
     ]
    }
   ],
   "source": [
    "journalHandler = JournalUploadHandler(uphandler)\n",
    "journalHandler.journalUpload(\"doaj.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
