class Handler(object): #this is the first class, all the others derive from this one 

    #creating the class 
    def __init__(self, dbPathOrUrl : str):
        self.dbPathOrUrl = dbPathOrUrl

    #creating the methods 
    def getDbPathOrUrl(self): 
        return self.dbPathOrUrl 

    def setDbPathOrUrl(self, pathOrUrl : str): 
        if self.dbPathOrUrl:
            self.dbPathOrUrl = pathOrUrl
            return True #in this case the boolean is useful to see if the method worked
        else: 
            return False 


class UploadHandler(Handler):

    def pushDataToDb(self, path: str): 
        if ".csv" in path: 
            handler = JournalUploadHandler(self.dbPathOrUrl)
            return handler.journalUpload(self, path) #calling the method after I called the subclass
        elif ".json" in path: 
            handler = CategoryUploadHandler(self.dbPathOrUrl)
            return handler.categoryUpload(self, path)
        else: 
            return False 


#first case: the path is of the relational database the json file
from json import load 
from sqlite3 import connect
import pandas as pd
from pandas import Series, DataFrame
class CategoryUploadHandler(UploadHandler): 
    
    def categoryUpload(self, path: str): 

        #creating the database 
        with connect("category.db") as con: 
            con.commit() #commit the current transactions to the database  

        with open(path, "r", encoding="utf-8") as c: 
            json = load(c) #reading the file 

            #internal identifier of all the items 
            for idx, item in enumerate(json): 
                item_internal_id = ("item-" + str(idx))
            
            #1. creating internal ids for each element: identifiers 
            identifiers = json["identifiers"] #selecting the identifiers 
            identifiers_df = DataFrame(identifiers) #creating a dataframe 

            identifiers_internal_ids = []
            for idx, row in identifiers_df.iterrows(): 
                identifiers_internal_ids.append(("internal_id-") +  str(idx)) #questa è una lista perchè sono tutti diversi non mi serve accedere al valore 

            identifiers_df.insert(0, "identifiers_id-", Series(identifiers_internal_ids, dtype="string"))
            #new column with the internal id 

            #2. creating internal ids for the categories, this is trickier because they have more than one value and they can have same id
            #i have to iterate thourg everuything but check if the "id" is the same, so it's useful to use a dictionary 
            categories = json["categories"] 
            categories_df = DataFrame(categories)
            
            dict_categories_internal_id = {} #using it to keep track of what we have
            categories_internal_id = [] #but it's more useful to still append it to the list, or i could have appended just the keys (so the new internal ids)
            #iterating like the other one 
            for idx, row in categories.iterrows(): 
                if row["id"] not in  dict_categories_internal_id: 
                     dict_categories_internal_id[row["id"]] = (("category-") + str(idx))
                categories_internal_id.append(dict_categories_internal_id[row["id"]])

            categories_df.insert(0, "categories_id-", Series(categories_internal_id, dtype="string"))
            
            #3. creating internal ids for areas, this is the same but without any more value 
            areas = json["areas"]
            areas_df = DataFrame(areas)

            dict_areas_internal_id = {}
            areas_internal_id = []
            for idx, row in areas.iterrows(): 
                area_section = row.iloc[0]
                if area_section not in dict_areas_internal_id: 
                    dict_areas_internal_id[area_section] = (("areas-") + str(idx))
                areas_internal_id.append(dict_areas_internal_id[area_section])

            areas_df.insert(0, "areas_id-", Series(areas_internal_id, dtype="string"))

        #adding them to the database 
        with connect("category.db") as con:
            identifiers_df.to_sql("identifiers", con, if_exists="replace", index=False)
            categories_df.to_sql("categories", con, if_exists="replace", index=False)
            areas_df.to_sql("areas", con, if_exists="replace", index=False)


            
#second case: the path is the one of a graph database, the csv file
from pandas import read_csv
from rdflib import Graph, URIRef, Literal, RDF 
from rdflib.plugins.stores.sparqlstore import SPARQLUpdateStore

class JournalUploadHandler(UploadHandler): 

    def journalUpload(self, path: str):   
        my_graph = Graph() #creating the database

        #classes
        IdentifiableEntity = URIRef("https://schema.org/Thing") #I made this super generic because id is already an attribute
        Journal = URIRef("https://schema.org/Periodical") 
        Category = URIRef("https://schema.org/category")
        Area = URIRef("https://www.wikidata.org/wiki/Q26256810") #I found the one of the topic because area has a different interpretation as more of a physical meaning 

        #predicate 
        hasCategory = URIRef("http://purl.org/dc/terms/subject")
        hasArea = URIRef("https://schema.org/about")

        #attributes related to classes 
        id = URIRef("https://schema.org/identifier")
        title = URIRef("https://schema.org/title")
        languages = URIRef("https://schema.org/inLanguage") 
        publisher = URIRef("https://schema.org/publisher")
        doajSeal = URIRef("https://schema.org/Certification") 
        licence = URIRef("https://schema.org/license")
        apc = URIRef("https://schema.org/isAccessibleForFree")
        quartile = URIRef("https://schema.org/ratingValue") 
        #the impact of the journal in the respecitive field so i use the ranking attribute


        #reading the csv  Journal title,Journal ISSN (print version),Journal EISSN (online version),Languages in which the journal accepts manuscripts,Publisher,DOAJ Seal,Journal license,APC
        journals = read_csv(path, 
                            keep_default_na=False, 
                            dtype={
                                "Journal title": "string",
                                "Journal ISSN": "string",
                                "Journal EISSN": "string",
                                "Languages in which the journal accepts manuscripts": "string",
                                "Publisher": "string",
                                "DOAJ seal": "string",
                                "Journal licence" : "string",
                                "APC": "boolean"
                            })

        #giving unique identifiers 
        base_url = "https://comp-data.github.io/res"
        
        journals_internal_id = []
        for idx, row in journals.iterrows(): 
            local_id = "journal-" + str(idx)
            subj = URIRef(base_url + local_id) #new local identifiers for each item in the graph database 

            my_graph.add((subj, RDF.type, Journal)) #the subject of the row is a journal 
            #checking every category in the row (which is none other than a panda Series, so a list of vocabularies)
            if row["Journal title"]: 
                my_graph.add(subj, title, Literal(row["Journal title"]))
            if row["Journal ISSN"]: 
                my_graph.add(subj, id, Literal(row["Journal ISSN"])) 
                #NEED TO DECIDE IF WE WANT TO CONSIDER BOTH AS ID, OR TO SEPATATE THEM (https://schema.org/issn) 
            if row["Journal EISSN"]: 
                my_graph.add(subj, id, Literal(row["Journal EISSN"])) 
            if row["Languages in which the journal accepts manuscripts"]: 
                my_graph.add(subj, languages, Literal(row["Languages in which the journal accepts manuscripts"])) 
            if row["Publisher"]: 
                my_graph.add(subj, publisher, Literal(row["Publisher"])) 
            if row["DOAJ seal"]: 
                my_graph.add(subj, doajSeal, Literal(row["DOAJ seal"])) 
            if row["Journal licence"]: 
                my_graph.add(subj, licence, Literal(row["Journal licence"])) 
            if row["APC"]: 
                my_graph.add(subj, apc, Literal(row["APC"])) 

        #opening the connection to upload the graph 
        store = SPARQLUpdateStore() #initializing it as an object 
        endpoint =  "http://192.168.1.14:9999/blazegraph/"

        store.open(endpoint, endpoint)

        for triple in my_graph.triples(None, None, None): 
            store.add(triple)

        #closing the connection when we finish 
        store.close()
        #SHOULD WE WRITE SOMETHING IF IT DOESN'T WORK? print("something went wrong")
        
